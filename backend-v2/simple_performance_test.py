#!/usr/bin/env python3
"""
ä¸‡ç‰©å¯è§†åŒ– v2.0 - ç®€åŒ–æ€§èƒ½æµ‹è¯•
ä¸ä¾èµ–å¤–éƒ¨åº“ï¼Œä¸“æ³¨äºAPIæ€§èƒ½æµ‹è¯•
"""

import asyncio
import json
import time
import requests
import random
import statistics
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any

class SimplePerformanceTester:
    def __init__(self, base_url: str = "http://localhost:9999"):
        self.base_url = base_url
        self.api_base = f"{base_url}/api/v2"
        self.performance_results = []

    def log_performance_test(self, test_name: str, results: Dict[str, Any]):
        """è®°å½•æ€§èƒ½æµ‹è¯•ç»“æœ"""
        self.performance_results.append({
            "test": test_name,
            "results": results,
            "timestamp": time.time()
        })

        print(f"\nğŸ“Š {test_name}")
        print(f"   ğŸ¯ è¯·æ±‚æ€»æ•°: {results.get('total_requests', 0)}")
        print(f"   âœ… æˆåŠŸè¯·æ±‚: {results.get('successful_requests', 0)}")
        print(f"   âŒ å¤±è´¥è¯·æ±‚: {results.get('failed_requests', 0)}")
        print(f"   â±ï¸  å¹³å‡å“åº”æ—¶é—´: {results.get('avg_response_time', 0):.3f}ç§’")
        print(f"   ğŸš€ æœ€å¿«å“åº”æ—¶é—´: {results.get('min_response_time', 0):.3f}ç§’")
        print(f"   ğŸŒ æœ€æ…¢å“åº”æ—¶é—´: {results.get('max_response_time', 0):.3f}ç§’")
        if 'qps' in results:
            print(f"   ğŸ“ˆ QPS (æ¯ç§’è¯·æ±‚æ•°): {results['qps']:.2f}")

    async def test_api_endpoint_performance(self, endpoint: str, method: str = 'GET',
                                      data: Dict = None, num_requests: int = 100) -> bool:
        """æµ‹è¯•APIç«¯ç‚¹æ€§èƒ½"""
        print(f"ğŸ¯ {endpoint} æ€§èƒ½æµ‹è¯• - {num_requests}ä¸ªè¯·æ±‚")

        response_times = []
        successful = 0
        failed = 0

        start_time = time.time()

        for i in range(num_requests):
            request_start = time.time()
            try:
                if method == 'GET':
                    response = requests.get(f"{self.api_base}{endpoint}")
                elif method == 'POST':
                    if data:
                        response = requests.post(f"{self.api_base}{endpoint}", json=data)
                    else:
                        response = requests.post(f"{self.api_base}{endpoint}")
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„HTTPæ–¹æ³•: {method}")

                request_time = time.time() - request_start
                response_times.append(request_time)

                if response.status_code == 200:
                    successful += 1
                else:
                    failed += 1

                # æ¯20ä¸ªè¯·æ±‚æ˜¾ç¤ºè¿›åº¦
                if (i + 1) % 20 == 0:
                    print(f"   è¿›åº¦: {i+1}/{num_requests} (æˆåŠŸ: {successful})")

            except Exception as e:
                failed += 1
                request_time = time.time() - request_start
                response_times.append(request_time)

        total_time = time.time() - start_time

        results = {
            "total_requests": num_requests,
            "successful_requests": successful,
            "failed_requests": failed,
            "avg_response_time": statistics.mean(response_times),
            "min_response_time": min(response_times),
            "max_response_time": max(response_times),
            "total_time": total_time,
            "qps": num_requests / total_time
        }

        self.log_performance_test(f"{endpoint} ç«¯ç‚¹æ€§èƒ½", results)
        return failed == 0

    async def test_concurrent_classify(self, num_concurrent: int = 10, total_requests: int = 50) -> bool:
        """æµ‹è¯•å¹¶å‘åˆ†ç±»è¯·æ±‚"""
        print(f"ğŸš€ å¹¶å‘åˆ†ç±»æµ‹è¯• - {num_concurrent}ä¸ªå¹¶å‘ï¼Œæ€»è®¡{total_requests}ä¸ªè¯·æ±‚")

        response_times = []
        successful = 0
        failed = 0

        def make_classify_request():
            """å•ä¸ªåˆ†ç±»è¯·æ±‚å‡½æ•°"""
            start_time = time.time()
            try:
                prompts = [
                    "æ­£æ€åˆ†å¸ƒ å‡å€¼0 æ ‡å‡†å·®1",
                    "æŠ›ç‰©çº¿å‡½æ•° y=xÂ²",
                    "äºŒé¡¹åˆ†å¸ƒ n=10 p=0.3",
                    "ä¸‰è§’å‡½æ•° sin(x)",
                    "çº¿æ€§å‡½æ•° y=2x+1"
                ]

                prompt = random.choice(prompts)
                response = requests.post(
                    f"{self.api_base}/classify",
                    json={"prompt": f"å¹¶å‘æµ‹è¯• {prompt}"}
                )

                request_time = time.time() - start_time
                return {
                    "success": response.status_code == 200,
                    "response_time": request_time,
                    "prompt": prompt
                }
            except Exception as e:
                request_time = time.time() - start_time
                return {
                    "success": False,
                    "response_time": request_time,
                    "error": str(e)
                }

        start_time = time.time()

        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¹¶å‘è¯·æ±‚
        with ThreadPoolExecutor(max_workers=num_concurrent) as executor:
            futures = [executor.submit(make_classify_request) for _ in range(total_requests)]

            for i, future in enumerate(as_completed(futures)):
                try:
                    result = future.result()
                    response_times.append(result["response_time"])

                    if result["success"]:
                        successful += 1
                    else:
                        failed += 1

                    # æ˜¾ç¤ºè¿›åº¦
                    if (i + 1) % 10 == 0:
                        print(f"   è¿›åº¦: {i+1}/{total_requests} (æˆåŠŸ: {successful})")

                except Exception as e:
                    failed += 1
                    print(f"   âš ï¸  è¯·æ±‚å¼‚å¸¸: {e}")

        total_time = time.time() - start_time

        results = {
            "total_requests": total_requests,
            "successful_requests": successful,
            "failed_requests": failed,
            "avg_response_time": statistics.mean(response_times),
            "min_response_time": min(response_times),
            "max_response_time": max(response_times),
            "total_time": total_time,
            "qps": total_requests / total_time,
            "concurrency": num_concurrent
        }

        self.log_performance_test(f"å¹¶å‘åˆ†ç±»æµ‹è¯• ({num_concurrent}å¹¶å‘)", results)
        return failed / total_requests < 0.1  # å…è®¸10%å¤±è´¥ç‡

    async def test_visualization_pipeline(self, num_tests: int = 10) -> bool:
        """æµ‹è¯•å®Œæ•´çš„å¯è§†åŒ–ç®¡é“"""
        print(f"ğŸ¨ å¯è§†åŒ–ç®¡é“æµ‹è¯• - {num_tests}ä¸ªå®Œæ•´æµç¨‹")

        generation_times = []
        successful = 0
        failed = 0

        start_time = time.time()

        prompts = [
            "æ­£æ€åˆ†å¸ƒ å‡å€¼0 æ ‡å‡†å·®1",
            "æŠ›ç‰©çº¿å‡½æ•° y=xÂ²",
            "ä¸‰è§’å‡½æ•° sin(x) cos(x)",
            "æŸ±çŠ¶å›¾æ•°æ®åˆ†å¸ƒ",
            "æ¦‚ç‡å¯†åº¦å‡½æ•°"
        ]

        for i in range(num_tests):
            prompt = prompts[i % len(prompts)]
            generation_start = time.time()

            try:
                print(f"   ğŸ“ ç”Ÿæˆè¯·æ±‚ {i+1}/{num_tests}: {prompt}")

                # æ­¥éª¤1: åˆ†ç±»
                classify_response = requests.post(
                    f"{self.api_base}/classify",
                    json={"prompt": prompt}
                )

                if classify_response.status_code != 200:
                    print(f"      âŒ åˆ†ç±»å¤±è´¥: {classify_response.status_code}")
                    failed += 1
                    continue

                classify_time = time.time() - generation_start

                # æ­¥éª¤2: ç”Ÿæˆ
                generate_response = requests.post(
                    f"{self.api_base}/generate",
                    json={"prompt": prompt}
                )

                if generate_response.status_code != 200:
                    print(f"      âŒ ç”Ÿæˆå¤±è´¥: {generate_response.status_code}")
                    failed += 1
                    continue

                generate_data = generate_response.json()
                generation_id = generate_data.get("generation_id")

                if not generation_id:
                    print(f"      âŒ æ²¡æœ‰ç”ŸæˆID")
                    failed += 1
                    continue

                generate_time = time.time() - generation_start

                # æ­¥éª¤3: ç­‰å¾…å®Œæˆï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œåªç­‰å¾…2ç§’ï¼‰
                await asyncio.sleep(2)

                generation_time = time.time() - generation_start
                generation_times.append(generation_time)
                successful += 1

                print(f"      âœ… å®Œæˆ: åˆ†ç±»{classify_time:.2f}s, ç”Ÿæˆ{generate_time:.2f}s")

            except Exception as e:
                print(f"      âŒ å¼‚å¸¸: {e}")
                failed += 1

        total_time = time.time() - start_time

        results = {
            "total_requests": num_tests,
            "successful_requests": successful,
            "failed_requests": failed,
            "avg_generation_time": statistics.mean(generation_times) if generation_times else 0,
            "min_generation_time": min(generation_times) if generation_times else 0,
            "max_generation_time": max(generation_times) if generation_times else 0,
            "total_time": total_time,
            "qps": successful / total_time if successful > 0 else 0
        }

        # è‡ªå®šä¹‰æ˜¾ç¤º
        print(f"\nğŸ“Š å¯è§†åŒ–ç®¡é“æµ‹è¯•")
        print(f"   ğŸ¯ æµ‹è¯•è¯·æ±‚æ€»æ•°: {results['total_requests']}")
        print(f"   âœ… æˆåŠŸç®¡é“: {results['successful_requests']}")
        print(f"   âŒ å¤±è´¥ç®¡é“: {results['failed_requests']}")
        print(f"   â±ï¸  å¹³å‡ç®¡é“æ—¶é—´: {results['avg_generation_time']:.2f}ç§’")
        print(f"   ğŸš€ æœ€å¿«ç®¡é“æ—¶é—´: {results['min_generation_time']:.2f}ç§’")
        print(f"   ğŸŒ æœ€æ…¢ç®¡é“æ—¶é—´: {results['max_generation_time']:.2f}ç§’")
        print(f"   ğŸ“ˆ ç®¡é“QPS: {results['qps']:.2f}")

        self.log_performance_test("å¯è§†åŒ–ç®¡é“æµ‹è¯•", results)
        return failed == 0

    async def test_stress_load(self, duration_seconds: int = 30) -> bool:
        """æµ‹è¯•å‹åŠ›è´Ÿè½½"""
        print(f"âš¡ å‹åŠ›è´Ÿè½½æµ‹è¯• - æŒç»­{duration_seconds}ç§’")

        start_time = time.time()
        end_time = start_time + duration_seconds

        request_count = 0
        successful = 0
        failed = 0
        response_times = []

        while time.time() < end_time:
            request_start = time.time()
            try:
                # æ··åˆä¸åŒç±»å‹çš„è¯·æ±‚
                request_type = random.choice(['health', 'classify', 'templates'])

                if request_type == 'health':
                    response = requests.get(f"{self.api_base}/health")
                elif request_type == 'classify':
                    response = requests.post(
                        f"{self.api_base}/classify",
                        json={"prompt": f"å‹åŠ›æµ‹è¯• {request_count}"}
                    )
                else:  # templates
                    response = requests.get(f"{self.api_base}/templates")

                request_time = time.time() - request_start
                response_times.append(request_time)

                if response.status_code == 200:
                    successful += 1
                else:
                    failed += 1

                request_count += 1

                # æ§åˆ¶è¯·æ±‚é¢‘ç‡
                await asyncio.sleep(0.05)  # 50msé—´éš”ï¼Œçº¦20 QPS

            except Exception as e:
                failed += 1
                request_time = time.time() - request_start
                response_times.append(request_time)

        actual_duration = time.time() - start_time

        results = {
            "total_requests": request_count,
            "successful_requests": successful,
            "failed_requests": failed,
            "avg_response_time": statistics.mean(response_times),
            "min_response_time": min(response_times),
            "max_response_time": max(response_times),
            "total_time": actual_duration,
            "qps": request_count / actual_duration
        }

        self.log_performance_test(f"å‹åŠ›è´Ÿè½½æµ‹è¯• ({duration_seconds}ç§’)", results)
        return failed / request_count < 0.05  # å…è®¸5%å¤±è´¥ç‡

    async def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•"""
        print("ğŸš€ ä¸‡ç‰©å¯è§†åŒ– v2.0 - æ€§èƒ½æµ‹è¯•")
        print("=" * 60)
        print()

        try:
            # æµ‹è¯•1: å¥åº·æ£€æŸ¥ç«¯ç‚¹æ€§èƒ½
            await self.test_api_endpoint_performance("/health", "GET", None, 50)

            # æµ‹è¯•2: åˆ†ç±»ç«¯ç‚¹æ€§èƒ½
            classify_data = {"prompt": "æ€§èƒ½æµ‹è¯•"}
            await self.test_api_endpoint_performance("/classify", "POST", classify_data, 30)

            # æµ‹è¯•3: å¹¶å‘åˆ†ç±»æµ‹è¯•
            await self.test_concurrent_classify(5, 25)
            await self.test_concurrent_classify(10, 50)

            # æµ‹è¯•4: å¯è§†åŒ–ç®¡é“æµ‹è¯•
            await self.test_visualization_pipeline(5)

            # æµ‹è¯•5: å‹åŠ›è´Ÿè½½æµ‹è¯•
            await self.test_stress_load(20)

        except Exception as e:
            print(f"âŒ æ€§èƒ½æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")

        return self.generate_performance_report()

    def generate_performance_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        total_tests = len(self.performance_results)
        successful_tests = sum(1 for r in self.performance_results
                              if r["results"].get("failed_requests", 0) == 0)

        print("\n" + "=" * 60)
        print("ğŸ“Š æ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
        print("=" * 60)
        print(f"æµ‹è¯•é¡¹ç›®: {total_tests}")
        print(f"æˆåŠŸæµ‹è¯•: {successful_tests}")
        print(f"æˆåŠŸç‡: {successful_tests/total_tests*100:.1f}%")
        print(f"æ€§èƒ½ç­‰çº§: {self.get_performance_grade()}")

        print("\nğŸ¯ è¯¦ç»†ç»“æœ:")
        for result in self.performance_results:
            test_name = result["test"]
            test_results = result["results"]
            qps = test_results.get("qps", 0)
            avg_time = test_results.get("avg_response_time", 0)

            print(f"   ğŸ“‹ {test_name}")
            print(f"      QPS: {qps:.2f}, å¹³å‡å“åº”: {avg_time:.3f}s")

        return {
            "total_tests": total_tests,
            "successful_tests": successful_tests,
            "performance_grade": self.get_performance_grade(),
            "detailed_results": self.performance_results
        }

    def get_performance_grade(self) -> str:
        """è¯„ä¼°æ€§èƒ½ç­‰çº§"""
        if not self.performance_results:
            return "æ— æ³•è¯„ä¼°"

        avg_qps = statistics.mean([
            r["results"].get("qps", 0)
            for r in self.performance_results
            if "qps" in r["results"]
        ])

        avg_response_time = statistics.mean([
            r["results"].get("avg_response_time", 0)
            for r in self.performance_results
        ])

        if avg_qps >= 15 and avg_response_time < 0.2:
            return "A+ (ä¼˜ç§€)"
        elif avg_qps >= 10 and avg_response_time < 0.5:
            return "A (è‰¯å¥½)"
        elif avg_qps >= 5 and avg_response_time < 1.0:
            return "B (ä¸€èˆ¬)"
        elif avg_qps >= 2 and avg_response_time < 2.0:
            return "C (è¾ƒå·®)"
        else:
            return "D (éœ€è¦ä¼˜åŒ–)"

async def main():
    """ä¸»å‡½æ•°"""
    tester = SimplePerformanceTester()

    try:
        await tester.run_all_tests()
    except KeyboardInterrupt:
        print("\nâ¹ï¸  æ€§èƒ½æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nğŸ’¥ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    asyncio.run(main())