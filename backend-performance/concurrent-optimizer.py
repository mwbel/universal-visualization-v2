"""
Concurrent Processing Optimizer
å¹¶å‘å¤„ç†ä¼˜åŒ–å™¨

åŠŸèƒ½åŒ…æ‹¬ï¼š
- å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—
- å¹¶å‘æ§åˆ¶
- ä»»åŠ¡è°ƒåº¦
- èµ„æºç®¡ç†
- æ€§èƒ½ç›‘æ§
"""

import asyncio
import time
import uuid
import json
import logging
from typing import Dict, List, Any, Optional, Callable, Union
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import aioredis
from functools import wraps
import inspect
import traceback

logger = logging.getLogger(__name__)

class TaskStatus(Enum):
    """ä»»åŠ¡çŠ¶æ€"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"
    RETRYING = "retrying"

class TaskPriority(Enum):
    """ä»»åŠ¡ä¼˜å…ˆçº§"""
    LOW = 1
    NORMAL = 2
    HIGH = 3
    CRITICAL = 4

@dataclass
class Task:
    """ä»»åŠ¡å¯¹è±¡"""
    id: str
    func: Callable
    args: tuple = field(default_factory=tuple)
    kwargs: dict = field(default_factory=dict)
    priority: TaskPriority = TaskPriority.NORMAL
    max_retries: int = 3
    retry_delay: float = 1.0
    timeout: Optional[float] = None
    created_at: datetime = field(default_factory=datetime.now)
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    status: TaskStatus = TaskStatus.PENDING
    result: Any = None
    error: Optional[Exception] = None
    retry_count: int = 0
    dependencies: List[str] = field(default_factory=list)

@dataclass
class WorkerConfig:
    """å·¥ä½œè¿›ç¨‹é…ç½®"""
    max_workers: int = 10
    max_concurrent_tasks: int = 50
    task_timeout: float = 300.0  # 5åˆ†é’Ÿ
    worker_idle_timeout: float = 60.0  # 1åˆ†é’Ÿ
    health_check_interval: float = 30.0  # 30ç§’

class TaskQueue:
    """å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—"""

    def __init__(self, redis_url: str = "redis://localhost:6379"):
        self.redis = None
        self.redis_url = redis_url
        self.queues = {
            TaskPriority.CRITICAL: asyncio.PriorityQueue(),
            TaskPriority.HIGH: asyncio.PriorityQueue(),
            TaskPriority.NORMAL: asyncio.PriorityQueue(),
            TaskPriority.LOW: asyncio.PriorityQueue()
        }
        self.running_tasks = {}
        self.completed_tasks = {}

    async def connect(self):
        """è¿æ¥Redis"""
        try:
            self.redis = await aioredis.from_url(self.redis_url, decode_responses=True)
            logger.info("âœ… ä»»åŠ¡é˜Ÿåˆ—Redisè¿æ¥æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ Redisè¿æ¥å¤±è´¥ï¼Œä½¿ç”¨å†…å­˜é˜Ÿåˆ—: {e}")

    async def put_task(self, task: Task) -> str:
        """æ·»åŠ ä»»åŠ¡åˆ°é˜Ÿåˆ—"""
        # ç”Ÿæˆä»»åŠ¡ID
        if not task.id:
            task.id = str(uuid.uuid4())

        # æ·»åŠ åˆ°å†…å­˜é˜Ÿåˆ—
        priority_value = -task.priority.value  # è´Ÿæ•°å®ç°é«˜ä¼˜å…ˆçº§
        await self.queues[task.priority].put((priority_value, task.id, task))

        # æŒä¹…åŒ–åˆ°Redis
        if self.redis:
            try:
                task_data = {
                    'id': task.id,
                    'priority': task.priority.value,
                    'created_at': task.created_at.isoformat(),
                    'status': task.status.value
                }
                await self.redis.hset(
                    f"task:{task.id}",
                    mapping=task_data
                )
                await self.redis.expire(f"task:{task.id}", 86400)  # 24å°æ—¶è¿‡æœŸ
            except Exception as e:
                logger.warning(f"Redisä»»åŠ¡å­˜å‚¨å¤±è´¥: {e}")

        logger.info(f"ğŸ“ ä»»åŠ¡å·²æ·»åŠ : {task.id} (ä¼˜å…ˆçº§: {task.priority.name})")
        return task.id

    async def get_task(self) -> Optional[Task]:
        """ä»é˜Ÿåˆ—è·å–ä»»åŠ¡"""
        # æŒ‰ä¼˜å…ˆçº§é¡ºåºæ£€æŸ¥é˜Ÿåˆ—
        for priority in sorted(TaskPriority, key=lambda x: x.value, reverse=True):
            try:
                if not self.queues[priority].empty():
                    _, task_id, task = await asyncio.wait_for(
                        self.queues[priority].get(),
                        timeout=0.1
                    )
                    return task
            except asyncio.TimeoutError:
                continue

        return None

    async def get_task_by_id(self, task_id: str) -> Optional[Task]:
        """æ ¹æ®IDè·å–ä»»åŠ¡"""
        # æ£€æŸ¥è¿è¡Œä¸­çš„ä»»åŠ¡
        if task_id in self.running_tasks:
            return self.running_tasks[task_id]

        # æ£€æŸ¥å·²å®Œæˆçš„ä»»åŠ¡
        if task_id in self.completed_tasks:
            return self.completed_tasks[task_id]

        # ä»Redisè·å–ä»»åŠ¡ä¿¡æ¯
        if self.redis:
            try:
                task_data = await self.redis.hgetall(f"task:{task_id}")
                if task_data:
                    # è¿™é‡Œéœ€è¦é‡å»ºå®Œæ•´çš„ä»»åŠ¡å¯¹è±¡
                    # å®é™…å®ç°ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„ååºåˆ—åŒ–é€»è¾‘
                    logger.info(f"ä»Redisè·å–ä»»åŠ¡: {task_id}")
            except Exception as e:
                logger.warning(f"Redisä»»åŠ¡è¯»å–å¤±è´¥: {e}")

        return None

    async def update_task_status(self, task_id: str, status: TaskStatus, result: Any = None, error: Exception = None):
        """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
        if task_id in self.running_tasks:
            task = self.running_tasks[task_id]
            task.status = status

            if status == TaskStatus.RUNNING:
                task.started_at = datetime.now()
            elif status in [TaskStatus.COMPLETED, TaskStatus.FAILED, TaskStatus.CANCELLED]:
                task.completed_at = datetime.now()
                if result is not None:
                    task.result = result
                if error is not None:
                    task.error = error

                # ç§»åŠ¨åˆ°å®Œæˆé˜Ÿåˆ—
                self.completed_tasks[task_id] = self.running_tasks.pop(task_id)

            # æ›´æ–°RedisçŠ¶æ€
            if self.redis:
                try:
                    await self.redis.hset(
                        f"task:{task_id}",
                        mapping={
                            'status': status.value,
                            'updated_at': datetime.now().isoformat()
                        }
                    )
                except Exception as e:
                    logger.warning(f"RedisçŠ¶æ€æ›´æ–°å¤±è´¥: {e}")

            logger.info(f"ğŸ”„ ä»»åŠ¡çŠ¶æ€æ›´æ–°: {task_id} -> {status.value}")

    async def get_queue_stats(self) -> Dict:
        """è·å–é˜Ÿåˆ—ç»Ÿè®¡"""
        stats = {
            'pending_tasks': {
                priority.name: queue.qsize()
                for priority, queue in self.queues.items()
            },
            'running_tasks': len(self.running_tasks),
            'completed_tasks': len(self.completed_tasks),
            'total_pending': sum(queue.qsize() for queue in self.queues.values())
        }

        # Redisç»Ÿè®¡
        if self.redis:
            try:
                redis_keys = await self.redis.keys("task:*")
                stats['redis_tasks'] = len(redis_keys)
            except Exception:
                stats['redis_tasks'] = 0

        return stats

class ConcurrentProcessor:
    """å¹¶å‘å¤„ç†å™¨"""

    def __init__(self, config: WorkerConfig = None):
        self.config = config or WorkerConfig()
        self.task_queue = TaskQueue()
        self.workers = []
        self.thread_executor = ThreadPoolExecutor(max_workers=self.config.max_workers)
        self.process_executor = ProcessPoolExecutor(max_workers=self.config.max_workers // 2)
        self.running = False
        self.stats = {
            'tasks_processed': 0,
            'tasks_failed': 0,
            'tasks_cancelled': 0,
            'avg_processing_time': 0.0,
            'start_time': None
        }

    async def start(self):
        """å¯åŠ¨å¹¶å‘å¤„ç†å™¨"""
        if self.running:
            logger.warning("âš ï¸ å¹¶å‘å¤„ç†å™¨å·²åœ¨è¿è¡Œ")
            return

        await self.task_queue.connect()
        self.running = True
        self.stats['start_time'] = datetime.now()

        # å¯åŠ¨å·¥ä½œåç¨‹
        for i in range(self.config.max_concurrent_tasks):
            worker = asyncio.create_task(self._worker(f"worker-{i}"))
            self.workers.append(worker)

        # å¯åŠ¨ç›‘æ§åç¨‹
        monitor = asyncio.create_task(self._monitor())
        self.workers.append(monitor)

        logger.info(f"ğŸš€ å¹¶å‘å¤„ç†å™¨å·²å¯åŠ¨: {self.config.max_concurrent_tasks} ä¸ªå·¥ä½œåç¨‹")

    async def stop(self):
        """åœæ­¢å¹¶å‘å¤„ç†å™¨"""
        if not self.running:
            return

        self.running = False

        # ç­‰å¾…æ‰€æœ‰å·¥ä½œåç¨‹å®Œæˆ
        await asyncio.gather(*self.workers, return_exceptions=True)

        # å…³é—­æ‰§è¡Œå™¨
        self.thread_executor.shutdown(wait=True)
        self.process_executor.shutdown(wait=True)

        logger.info("ğŸ›‘ å¹¶å‘å¤„ç†å™¨å·²åœæ­¢")

    async def _worker(self, name: str):
        """å·¥ä½œåç¨‹"""
        logger.info(f"ğŸ‘· å·¥ä½œåç¨‹å¯åŠ¨: {name}")

        while self.running:
            try:
                # è·å–ä»»åŠ¡
                task = await asyncio.wait_for(
                    self.task_queue.get_task(),
                    timeout=self.config.worker_idle_timeout
                )

                if task:
                    await self._execute_task(task, name)
                else:
                    # ç©ºé—²ç­‰å¾…
                    await asyncio.sleep(0.1)

            except asyncio.TimeoutError:
                logger.debug(f"ğŸ’¤ å·¥ä½œåç¨‹ç©ºé—²è¶…æ—¶: {name}")
            except Exception as e:
                logger.error(f"âŒ å·¥ä½œåç¨‹å¼‚å¸¸: {name} - {e}")

        logger.info(f"ğŸ‘· å·¥ä½œåç¨‹åœæ­¢: {name}")

    async def _execute_task(self, task: Task, worker_name: str):
        """æ‰§è¡Œä»»åŠ¡"""
        task_id = task.id

        # æ£€æŸ¥ä»»åŠ¡ä¾èµ–
        if not await self._check_dependencies(task):
            logger.info(f"â³ ä»»åŠ¡ä¾èµ–æœªæ»¡è¶³ï¼Œé‡æ–°å…¥é˜Ÿ: {task_id}")
            await self.task_queue.put_task(task)
            return

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        await self.task_queue.update_task_status(task_id, TaskStatus.RUNNING)
        self.task_queue.running_tasks[task_id] = task

        try:
            # æ‰§è¡Œä»»åŠ¡
            start_time = time.time()

            if inspect.iscoroutinefunction(task.func):
                # å¼‚æ­¥å‡½æ•°
                result = await asyncio.wait_for(
                    task.func(*task.args, **task.kwargs),
                    timeout=task.timeout or self.config.task_timeout
                )
            else:
                # åŒæ­¥å‡½æ•°ï¼Œåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œ
                loop = asyncio.get_event_loop()
                result = await asyncio.wait_for(
                    loop.run_in_executor(
                        self.thread_executor,
                        lambda: task.func(*task.args, **task.kwargs)
                    ),
                    timeout=task.timeout or self.config.task_timeout
                )

            execution_time = time.time() - start_time

            # æ›´æ–°ç»Ÿè®¡
            self.stats['tasks_processed'] += 1
            self._update_avg_processing_time(execution_time)

            # ä»»åŠ¡å®Œæˆ
            await self.task_queue.update_task_status(task_id, TaskStatus.COMPLETED, result=result)
            logger.info(f"âœ… ä»»åŠ¡å®Œæˆ: {task_id} ({worker_name}) - è€—æ—¶ {execution_time:.2f}s")

        except asyncio.TimeoutError:
            logger.warning(f"â° ä»»åŠ¡è¶…æ—¶: {task_id}")
            await self._handle_task_timeout(task)
        except Exception as e:
            logger.error(f"âŒ ä»»åŠ¡å¤±è´¥: {task_id} - {e}")
            await self._handle_task_failure(task, e)

    async def _check_dependencies(self, task: Task) -> bool:
        """æ£€æŸ¥ä»»åŠ¡ä¾èµ–"""
        for dep_id in task.dependencies:
            dep_task = await self.task_queue.get_task_by_id(dep_id)
            if not dep_task or dep_task.status != TaskStatus.COMPLETED:
                return False
        return True

    async def _handle_task_timeout(self, task: Task):
        """å¤„ç†ä»»åŠ¡è¶…æ—¶"""
        task.retry_count += 1

        if task.retry_count <= task.max_retries:
            # é‡è¯•ä»»åŠ¡
            await asyncio.sleep(task.retry_delay)
            task.status = TaskStatus.RETRYING
            await self.task_queue.put_task(task)
            logger.info(f"ğŸ”„ ä»»åŠ¡é‡è¯•: {task.id} (ç¬¬{task.retry_count}æ¬¡)")
        else:
            # æ ‡è®°ä¸ºå¤±è´¥
            await self.task_queue.update_task_status(
                task.id,
                TaskStatus.FAILED,
                error=TimeoutError(f"ä»»åŠ¡è¶…æ—¶: {task.timeout}s")
            )
            self.stats['tasks_failed'] += 1

    async def _handle_task_failure(self, task: Task, error: Exception):
        """å¤„ç†ä»»åŠ¡å¤±è´¥"""
        task.retry_count += 1
        task.error = error

        if task.retry_count <= task.max_retries:
            # é‡è¯•ä»»åŠ¡
            await asyncio.sleep(task.retry_delay)
            task.status = TaskStatus.RETRYING
            await self.task_queue.put_task(task)
            logger.info(f"ğŸ”„ ä»»åŠ¡é‡è¯•: {task.id} (ç¬¬{task.retry_count}æ¬¡) - {error}")
        else:
            # æ ‡è®°ä¸ºå¤±è´¥
            await self.task_queue.update_task_status(
                task.id,
                TaskStatus.FAILED,
                error=error
            )
            self.stats['tasks_failed'] += 1

    def _update_avg_processing_time(self, execution_time: float):
        """æ›´æ–°å¹³å‡å¤„ç†æ—¶é—´"""
        total_tasks = self.stats['tasks_processed']
        if total_tasks == 1:
            self.stats['avg_processing_time'] = execution_time
        else:
            self.stats['avg_processing_time'] = (
                (self.stats['avg_processing_time'] * (total_tasks - 1) + execution_time) / total_tasks
            )

    async def _monitor(self):
        """ç›‘æ§åç¨‹"""
        while self.running:
            try:
                # è·å–é˜Ÿåˆ—ç»Ÿè®¡
                queue_stats = await self.task_queue.get_queue_stats()

                # è®¡ç®—è¿è¡Œæ—¶é—´
                runtime = (datetime.now() - self.stats['start_time']).total_seconds()

                # è®¡ç®—ååé‡
                throughput = self.stats['tasks_processed'] / runtime if runtime > 0 else 0

                logger.info(f"ğŸ“Š æ€§èƒ½ç»Ÿè®¡ - å¤„ç†å™¨çŠ¶æ€: è¿è¡Œä¸­ {runtime:.1f}s, "
                           f"å·²å¤„ç† {self.stats['tasks_processed']} ä¸ªä»»åŠ¡, "
                           f"å¤±è´¥ {self.stats['tasks_failed']} ä¸ª, "
                           f"ååé‡ {throughput:.2f} ä»»åŠ¡/ç§’, "
                           f"å¾…å¤„ç† {queue_stats['total_pending']} ä¸ª")

                # æ£€æŸ¥å¥åº·çŠ¶æ€
                await self._health_check()

                await asyncio.sleep(self.config.health_check_interval)

            except Exception as e:
                logger.error(f"âŒ ç›‘æ§åç¨‹å¼‚å¸¸: {e}")
                await asyncio.sleep(5)

    async def _health_check(self):
        """å¥åº·æ£€æŸ¥"""
        # æ£€æŸ¥å†…å­˜ä½¿ç”¨
        try:
            import psutil
            memory_percent = psutil.virtual_memory().percent
            if memory_percent > 90:
                logger.warning(f"âš ï¸ å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {memory_percent:.1f}%")
        except ImportError:
            pass

        # æ£€æŸ¥ä»»åŠ¡ç§¯å‹
        queue_stats = await self.task_queue.get_queue_stats()
        if queue_stats['total_pending'] > 1000:
            logger.warning(f"âš ï¸ ä»»åŠ¡ç§¯å‹ä¸¥é‡: {queue_stats['total_pending']} ä¸ªå¾…å¤„ç†ä»»åŠ¡")

    async def submit_task(self, func: Callable, *args,
                         priority: TaskPriority = TaskPriority.NORMAL,
                         max_retries: int = 3,
                         timeout: Optional[float] = None,
                         dependencies: List[str] = None,
                         **kwargs) -> str:
        """æäº¤ä»»åŠ¡"""
        task = Task(
            id=str(uuid.uuid4()),
            func=func,
            args=args,
            kwargs=kwargs,
            priority=priority,
            max_retries=max_retries,
            timeout=timeout,
            dependencies=dependencies or []
        )

        return await self.task_queue.put_task(task)

    async def get_task_status(self, task_id: str) -> Optional[Dict]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        task = await self.task_queue.get_task_by_id(task_id)
        if not task:
            return None

        return {
            'id': task.id,
            'status': task.status.value,
            'created_at': task.created_at.isoformat(),
            'started_at': task.started_at.isoformat() if task.started_at else None,
            'completed_at': task.completed_at.isoformat() if task.completed_at else None,
            'priority': task.priority.name,
            'retry_count': task.retry_count,
            'max_retries': task.max_retries,
            'result': task.result if task.status == TaskStatus.COMPLETED else None,
            'error': str(task.error) if task.error else None
        }

    async def cancel_task(self, task_id: str) -> bool:
        """å–æ¶ˆä»»åŠ¡"""
        task = await self.task_queue.get_task_by_id(task_id)
        if task and task.status == TaskStatus.PENDING:
            await self.task_queue.update_task_status(task_id, TaskStatus.CANCELLED)
            self.stats['tasks_cancelled'] += 1
            logger.info(f"ğŸš« ä»»åŠ¡å·²å–æ¶ˆ: {task_id}")
            return True
        return False

    async def get_performance_stats(self) -> Dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        queue_stats = await self.task_queue.get_queue_stats()
        runtime = (datetime.now() - self.stats['start_time']).total_seconds() if self.stats['start_time'] else 0

        return {
            'runtime_seconds': runtime,
            'tasks_processed': self.stats['tasks_processed'],
            'tasks_failed': self.stats['tasks_failed'],
            'tasks_cancelled': self.stats['tasks_cancelled'],
            'avg_processing_time': self.stats['avg_processing_time'],
            'throughput_per_second': self.stats['tasks_processed'] / runtime if runtime > 0 else 0,
            'success_rate': (self.stats['tasks_processed'] / (self.stats['tasks_processed'] + self.stats['tasks_failed']) * 100) if (self.stats['tasks_processed'] + self.stats['tasks_failed']) > 0 else 0,
            'queue_stats': queue_stats,
            'worker_count': len(self.workers),
            'max_workers': self.config.max_workers,
            'max_concurrent_tasks': self.config.max_concurrent_tasks
        }

def concurrent_task(priority: TaskPriority = TaskPriority.NORMAL,
                   max_retries: int = 3,
                   timeout: Optional[float] = None):
    """å¹¶å‘ä»»åŠ¡è£…é¥°å™¨"""
    def decorator(func: Callable):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # å¦‚æœå·²ç»åœ¨å¹¶å‘å¤„ç†å™¨ç¯å¢ƒä¸­ï¼Œç›´æ¥æ‰§è¡Œ
            if 'concurrent_processor' in kwargs:
                processor = kwargs['concurrent_processor']
                return await processor.submit_task(
                    func, *args,
                    priority=priority,
                    max_retries=max_retries,
                    timeout=timeout,
                    **{k: v for k, v in kwargs.items() if k != 'concurrent_processor'}
                )
            else:
                # å¦åˆ™ç›´æ¥æ‰§è¡Œ
                if inspect.iscoroutinefunction(func):
                    return await func(*args, **kwargs)
                else:
                    return func(*args, **kwargs)

        return wrapper
    return decorator

# ä½¿ç”¨ç¤ºä¾‹
"""
# åˆ›å»ºå¹¶å‘å¤„ç†å™¨
processor = ConcurrentProcessor()
await processor.start()

# å®šä¹‰ä»»åŠ¡å‡½æ•°
@concurrent_task(priority=TaskPriority.HIGH, max_retries=3)
async def generate_visualization(data: dict):
    # è€—æ—¶çš„å¯è§†åŒ–ç”Ÿæˆé€»è¾‘
    await asyncio.sleep(2)
    return {"status": "completed", "data": data}

# æäº¤ä»»åŠ¡
task_id = await processor.submit_task(generate_visualization, {"type": "chart"})

# æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
status = await processor.get_task_status(task_id)

# è·å–æ€§èƒ½ç»Ÿè®¡
stats = await processor.get_performance_stats()
"""

# æ‰¹é‡ä»»åŠ¡å¤„ç†å™¨
class BatchProcessor:
    """æ‰¹é‡ä»»åŠ¡å¤„ç†å™¨"""

    def __init__(self, concurrent_processor: ConcurrentProcessor):
        self.processor = concurrent_processor

    async def process_batch(self, func: Callable, items: List[Any],
                           batch_size: int = 10,
                           max_concurrent_batches: int = 5) -> List[str]:
        """æ‰¹é‡å¤„ç†ä»»åŠ¡"""
        task_ids = []

        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(items), batch_size):
            batch = items[i:i + batch_size]
            task_id = await self.processor.submit_task(func, batch)
            task_ids.append(task_id)

            # æ§åˆ¶å¹¶å‘æ‰¹æ¬¡æ•°é‡
            if len(task_ids) >= max_concurrent_batches:
                # ç­‰å¾…ä¸€äº›ä»»åŠ¡å®Œæˆ
                await self._wait_for_tasks_completion(task_ids[:-max_concurrent_batches//2])

        return task_ids

    async def _wait_for_tasks_completion(self, task_ids: List[str], timeout: float = 300.0):
        """ç­‰å¾…ä»»åŠ¡å®Œæˆ"""
        start_time = time.time()

        while task_ids and time.time() - start_time < timeout:
            completed_tasks = []

            for task_id in task_ids:
                status = await self.processor.get_task_status(task_id)
                if status and status['status'] in ['completed', 'failed', 'cancelled']:
                    completed_tasks.append(task_id)

            # ç§»é™¤å·²å®Œæˆçš„ä»»åŠ¡
            for task_id in completed_tasks:
                task_ids.remove(task_id)

            if task_ids:
                await asyncio.sleep(1)

        return len(task_ids) == 0

# å…¨å±€å®ä¾‹ï¼ˆå¯é€‰ï¼‰
_global_processor = None

async def get_global_processor() -> ConcurrentProcessor:
    """è·å–å…¨å±€å¹¶å‘å¤„ç†å™¨"""
    global _global_processor
    if _global_processor is None:
        _global_processor = ConcurrentProcessor()
        await _global_processor.start()
    return _global_processor