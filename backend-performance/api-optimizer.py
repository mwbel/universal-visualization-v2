"""
API Performance Optimizer
APIæ€§èƒ½ä¼˜åŒ–å™¨

åŠŸèƒ½åŒ…æ‹¬ï¼š
- å“åº”ç¼“å­˜
- è¯·æ±‚å»é‡
- å¹¶å‘æ§åˆ¶
- å“åº”å‹ç¼©
- é™æµä¿æŠ¤
"""

import asyncio
import hashlib
import json
import gzip
import time
from functools import wraps
from typing import Dict, Any, Optional, Callable, List
from dataclasses import dataclass
from collections import defaultdict
import aioredis
from fastapi import Request, Response
from fastapi.responses import JSONResponse
import logging

logger = logging.getLogger(__name__)

@dataclass
class CacheConfig:
    """ç¼“å­˜é…ç½®"""
    ttl: int = 300  # 5åˆ†é’Ÿ
    max_size: int = 1000
    key_prefix: str = "api_cache"
    compress_threshold: int = 1024  # 1KBä»¥ä¸Šå‹ç¼©

@dataclass
class RateLimitConfig:
    """é™æµé…ç½®"""
    requests_per_minute: int = 60
    requests_per_hour: int = 1000
    burst_size: int = 10

class APICache:
    """APIå“åº”ç¼“å­˜"""

    def __init__(self, redis_url: str = "redis://localhost:6379", config: CacheConfig = None):
        self.redis = None
        self.redis_url = redis_url
        self.config = config or CacheConfig()
        self.local_cache = {}  # æœ¬åœ°ç¼“å­˜ä½œä¸ºåå¤‡
        self.local_cache_timestamps = {}

    async def connect(self):
        """è¿æ¥Redis"""
        try:
            self.redis = await aioredis.from_url(self.redis_url, decode_responses=True)
            logger.info("âœ… Redisç¼“å­˜è¿æ¥æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ Redisè¿æ¥å¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°ç¼“å­˜: {e}")
            self.redis = None

    def _generate_cache_key(self, request: Request, params: Dict = None) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # åŒ…å«æ–¹æ³•ã€è·¯å¾„ã€æŸ¥è¯¢å‚æ•°ã€è¯·æ±‚ä½“
        key_data = {
            "method": request.method,
            "url": str(request.url),
            "headers": dict(request.headers),
            "params": params or {}
        }

        key_string = json.dumps(key_data, sort_keys=True)
        key_hash = hashlib.md5(key_string.encode()).hexdigest()
        return f"{self.config.key_prefix}:{key_hash}"

    async def get(self, request: Request, params: Dict = None) -> Optional[Dict]:
        """è·å–ç¼“å­˜å“åº”"""
        cache_key = self._generate_cache_key(request, params)

        # å…ˆå°è¯•Redis
        if self.redis:
            try:
                cached_data = await self.redis.get(cache_key)
                if cached_data:
                    data = json.loads(cached_data)
                    logger.info(f"ğŸ’¾ Redisç¼“å­˜å‘½ä¸­: {cache_key}")
                    return data
            except Exception as e:
                logger.warning(f"Redisè¯»å–å¤±è´¥: {e}")

        # å›é€€åˆ°æœ¬åœ°ç¼“å­˜
        if cache_key in self.local_cache:
            timestamp = self.local_cache_timestamps.get(cache_key, 0)
            if time.time() - timestamp < self.config.ttl:
                logger.info(f"ğŸ’¾ æœ¬åœ°ç¼“å­˜å‘½ä¸­: {cache_key}")
                return self.local_cache[cache_key]
            else:
                # è¿‡æœŸåˆ é™¤
                del self.local_cache[cache_key]
                del self.local_cache_timestamps[cache_key]

        return None

    async def set(self, request: Request, response_data: Dict, params: Dict = None):
        """è®¾ç½®ç¼“å­˜å“åº”"""
        cache_key = self._generate_cache_key(request, params)

        # å‹ç¼©å¤§æ•°æ®
        if len(json.dumps(response_data)) > self.config.compress_threshold:
            response_data["_compressed"] = True
            response_data["_original_size"] = len(json.dumps(response_data))

        # å­˜å‚¨åˆ°Redis
        if self.redis:
            try:
                await self.redis.setex(
                    cache_key,
                    self.config.ttl,
                    json.dumps(response_data)
                )
                logger.info(f"ğŸ’¾ Redisç¼“å­˜å­˜å‚¨: {cache_key}")
                return
            except Exception as e:
                logger.warning(f"Rediså­˜å‚¨å¤±è´¥: {e}")

        # å›é€€åˆ°æœ¬åœ°ç¼“å­˜
        self.local_cache[cache_key] = response_data
        self.local_cache_timestamps[cache_key] = time.time()

        # æ¸…ç†è¿‡æœŸçš„æœ¬åœ°ç¼“å­˜
        await self._cleanup_local_cache()

    async def _cleanup_local_cache(self):
        """æ¸…ç†è¿‡æœŸçš„æœ¬åœ°ç¼“å­˜"""
        current_time = time.time()
        expired_keys = []

        for key, timestamp in self.local_cache_timestamps.items():
            if current_time - timestamp > self.config.ttl:
                expired_keys.append(key)

        for key in expired_keys:
            del self.local_cache[key]
            del self.local_cache_timestamps[key]

        # é™åˆ¶æœ¬åœ°ç¼“å­˜å¤§å°
        if len(self.local_cache) > self.config.max_size:
            # åˆ é™¤æœ€æ—§çš„ç¼“å­˜
            oldest_keys = sorted(
                self.local_cache_timestamps.items(),
                key=lambda x: x[1]
            )[:len(self.local_cache) - self.config.max_size]

            for key, _ in oldest_keys:
                del self.local_cache[key]
                del self.local_cache_timestamps[key]

    async def invalidate(self, pattern: str = None):
        """æ¸…é™¤ç¼“å­˜"""
        if self.redis:
            try:
                if pattern:
                    keys = await self.redis.keys(f"{self.config.key_prefix}:{pattern}*")
                    if keys:
                        await self.redis.delete(*keys)
                        logger.info(f"ğŸ—‘ï¸ æ¸…é™¤ç¼“å­˜æ¨¡å¼: {pattern}")
                else:
                    # æ¸…é™¤æ‰€æœ‰åº”ç”¨ç¼“å­˜
                    keys = await self.redis.keys(f"{self.config.key_prefix}:*")
                    if keys:
                        await self.redis.delete(*keys)
                        logger.info("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰ç¼“å­˜")
            except Exception as e:
                logger.warning(f"Redisæ¸…é™¤å¤±è´¥: {e}")

        # æ¸…é™¤æœ¬åœ°ç¼“å­˜
        if pattern:
            keys_to_remove = [k for k in self.local_cache.keys() if pattern in k]
            for key in keys_to_remove:
                del self.local_cache[key]
                del self.local_cache_timestamps[key]
            logger.info(f"ğŸ—‘ï¸ æ¸…é™¤æœ¬åœ°ç¼“å­˜æ¨¡å¼: {pattern}")
        else:
            self.local_cache.clear()
            self.local_cache_timestamps.clear()
            logger.info("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰æœ¬åœ°ç¼“å­˜")

class RateLimiter:
    """APIé™æµå™¨"""

    def __init__(self, redis_url: str = "redis://localhost:6379", config: RateLimitConfig = None):
        self.redis = None
        self.redis_url = redis_url
        self.config = config or RateLimitConfig()
        self.local_counters = defaultdict(list)

    async def connect(self):
        """è¿æ¥Redis"""
        try:
            self.redis = await aioredis.from_url(self.redis_url, decode_responses=True)
            logger.info("âœ… Redisé™æµè¿æ¥æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ Redisé™æµè¿æ¥å¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°è®¡æ•°: {e}")
            self.redis = None

    def _get_client_key(self, request: Request) -> str:
        """è·å–å®¢æˆ·ç«¯æ ‡è¯†"""
        # ä¼˜å…ˆä½¿ç”¨APIå¯†é’¥ï¼Œç„¶åæ˜¯IPåœ°å€
        api_key = request.headers.get("X-API-Key")
        if api_key:
            return f"api_key:{api_key}"

        # è·å–çœŸå®IPï¼ˆè€ƒè™‘ä»£ç†ï¼‰
        forwarded_for = request.headers.get("X-Forwarded-For")
        if forwarded_for:
            return f"ip:{forwarded_for.split(',')[0].strip()}"

        return f"ip:{request.client.host}"

    async def is_allowed(self, request: Request) -> tuple[bool, Dict]:
        """æ£€æŸ¥æ˜¯å¦å…è®¸è¯·æ±‚"""
        client_key = self._get_client_key(request)
        current_time = int(time.time())

        # æ£€æŸ¥åˆ†é’Ÿçº§é™æµ
        minute_key = f"rate_limit:minute:{client_key}:{current_time // 60}"
        minute_count = await self._get_count(minute_key, 60)

        if minute_count >= self.config.requests_per_minute:
            return False, {
                "error": "Rate limit exceeded",
                "limit": self.config.requests_per_minute,
                "window": "1 minute",
                "retry_after": 60 - (current_time % 60)
            }

        # æ£€æŸ¥å°æ—¶çº§é™æµ
        hour_key = f"rate_limit:hour:{client_key}:{current_time // 3600}"
        hour_count = await self._get_count(hour_key, 3600)

        if hour_count >= self.config.requests_per_hour:
            return False, {
                "error": "Rate limit exceeded",
                "limit": self.config.requests_per_hour,
                "window": "1 hour",
                "retry_after": 3600 - (current_time % 3600)
            }

        # æ£€æŸ¥çªå‘é™æµ
        burst_key = f"rate_limit:burst:{client_key}"
        burst_count = await self._get_burst_count(burst_key)

        if burst_count >= self.config.burst_size:
            return False, {
                "error": "Burst rate limit exceeded",
                "limit": self.config.burst_size,
                "window": "10 seconds",
                "retry_after": 10
            }

        # å¢åŠ è®¡æ•°
        await self._increment_count(minute_key, 60)
        await self._increment_count(hour_key, 3600)
        await self._increment_burst_count(burst_key)

        return True, {}

    async def _get_count(self, key: str, window: int) -> int:
        """è·å–è®¡æ•°"""
        if self.redis:
            try:
                count = await self.redis.get(key)
                return int(count) if count else 0
            except Exception as e:
                logger.warning(f"Redisè®¡æ•°è¯»å–å¤±è´¥: {e}")

        # å›é€€åˆ°æœ¬åœ°è®¡æ•°
        current_time = time.time()
        cutoff_time = current_time - window

        # æ¸…ç†è¿‡æœŸè®¡æ•°
        if key in self.local_counters:
            self.local_counters[key] = [
                timestamp for timestamp in self.local_counters[key]
                if timestamp > cutoff_time
            ]
            return len(self.local_counters[key])

        return 0

    async def _increment_count(self, key: str, window: int):
        """å¢åŠ è®¡æ•°"""
        if self.redis:
            try:
                await self.redis.incr(key)
                await self.redis.expire(key, window)
                return
            except Exception as e:
                logger.warning(f"Redisè®¡æ•°å†™å…¥å¤±è´¥: {e}")

        # å›é€€åˆ°æœ¬åœ°è®¡æ•°
        if key not in self.local_counters:
            self.local_counters[key] = []
        self.local_counters[key].append(time.time())

    async def _get_burst_count(self, key: str) -> int:
        """è·å–çªå‘è®¡æ•°"""
        if self.redis:
            try:
                count = await self.redis.get(key)
                return int(count) if count else 0
            except Exception:
                pass

        # ç®€åŒ–çš„æœ¬åœ°çªå‘è®¡æ•°
        return len([t for t in self.local_counters.get(key, []) if time.time() - t < 10])

    async def _increment_burst_count(self, key: str):
        """å¢åŠ çªå‘è®¡æ•°"""
        if self.redis:
            try:
                await self.redis.incr(key)
                await self.redis.expire(key, 10)
                return
            except Exception:
                pass

        # æœ¬åœ°çªå‘è®¡æ•°
        if key not in self.local_counters:
            self.local_counters[key] = []
        self.local_counters[key].append(time.time())

class APIOptimizer:
    """APIæ€§èƒ½ä¼˜åŒ–å™¨ä¸»ç±»"""

    def __init__(self, redis_url: str = "redis://localhost:6379"):
        self.cache = APICache(redis_url)
        self.rate_limiter = RateLimiter(redis_url)
        self.request_deduplicator = RequestDeduplicator()

    async def initialize(self):
        """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶"""
        await self.cache.connect()
        await self.rate_limiter.connect()
        logger.info("ğŸš€ APIä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")

    def cache_response(self, ttl: int = 300, key_prefix: str = None):
        """ç¼“å­˜å“åº”è£…é¥°å™¨"""
        def decorator(func: Callable):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                # æ„å»ºæ¨¡æ‹Ÿè¯·æ±‚å¯¹è±¡
                request = kwargs.get('request')
                if not request:
                    return await func(*args, **kwargs)

                # æ£€æŸ¥ç¼“å­˜
                cached_response = await self.cache.get(request, kwargs)
                if cached_response:
                    # å¦‚æœæ˜¯å‹ç¼©æ•°æ®ï¼Œè§£å‹
                    if cached_response.get("_compressed"):
                        cached_response.pop("_compressed", None)
                        cached_response.pop("_original_size", None)

                    return JSONResponse(
                        content=cached_response,
                        headers={"X-Cache": "HIT"}
                    )

                # æ‰§è¡ŒåŸå‡½æ•°
                response = await func(*args, **kwargs)

                # ç¼“å­˜å“åº”
                if hasattr(response, 'body_dict'):
                    await self.cache.set(request, response.body_dict, kwargs)
                elif isinstance(response, dict):
                    await self.cache.set(request, response, kwargs)

                return response

            return wrapper
        return decorator

    def rate_limit(self, requests_per_minute: int = None, requests_per_hour: int = None):
        """é™æµè£…é¥°å™¨"""
        def decorator(func: Callable):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                request = kwargs.get('request')
                if not request:
                    return await func(*args, **kwargs)

                # æ£€æŸ¥é™æµ
                allowed, error_info = await self.rate_limiter.is_allowed(request)

                if not allowed:
                    return JSONResponse(
                        status_code=429,
                        content=error_info,
                        headers={
                            "X-RateLimit-Limit": str(requests_per_minute or self.rate_limiter.config.requests_per_minute),
                            "X-RateLimit-Remaining": "0",
                            "X-RateLimit-Retry": str(error_info.get("retry_after", 60))
                        }
                    )

                # æ‰§è¡ŒåŸå‡½æ•°
                response = await func(*args, **kwargs)

                # æ·»åŠ é™æµå¤´ä¿¡æ¯
                if hasattr(response, 'headers'):
                    response.headers["X-RateLimit-Limit"] = str(requests_per_minute or self.rate_limiter.config.requests_per_minute)

                return response

            return wrapper
        return decorator

    def deduplicate_request(self):
        """è¯·æ±‚å»é‡è£…é¥°å™¨"""
        def decorator(func: Callable):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                request = kwargs.get('request')
                if not request:
                    return await func(*args, **kwargs)

                # æ£€æŸ¥æ˜¯å¦æ˜¯é‡å¤è¯·æ±‚
                result = await self.request_deduplicator.check_duplicate(request)
                if result:
                    return result

                # æ ‡è®°è¯·æ±‚å¼€å§‹å¤„ç†
                await self.request_deduplicator.mark_processing(request)

                try:
                    # æ‰§è¡ŒåŸå‡½æ•°
                    response = await func(*args, **kwargs)

                    # ç¼“å­˜ç»“æœä»¥ä¾›é‡å¤è¯·æ±‚ä½¿ç”¨
                    await self.request_deduplicator.cache_result(request, response)

                    return response
                finally:
                    # æ ‡è®°è¯·æ±‚å¤„ç†å®Œæˆ
                    await self.request_deduplicator.mark_completed(request)

            return wrapper
        return decorator

    def compress_response(self, threshold: int = 1024):
        """å“åº”å‹ç¼©è£…é¥°å™¨"""
        def decorator(func: Callable):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                response = await func(*args, **kwargs)

                # æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©
                if hasattr(response, 'body'):
                    content_length = len(response.body)
                    if content_length > threshold:
                        # å‹ç¼©å“åº”
                        compressed_body = gzip.compress(response.body)

                        # æ›´æ–°å“åº”
                        response.body = compressed_body
                        response.headers["Content-Encoding"] = "gzip"
                        response.headers["Content-Length"] = str(len(compressed_body))
                        response.headers["X-Compressed"] = "true"
                        response.headers["X-Original-Size"] = str(content_length)

                return response

            return wrapper
        return decorator

class RequestDeduplicator:
    """è¯·æ±‚å»é‡å™¨"""

    def def __init__(self):
        self.processing_requests = set()
        self.request_results = {}
        self.request_timestamps = {}

    def _generate_request_key(self, request: Request) -> str:
        """ç”Ÿæˆè¯·æ±‚å”¯ä¸€æ ‡è¯†"""
        key_data = {
            "method": request.method,
            "url": str(request.url),
            "headers": dict(request.headers)
        }
        key_string = json.dumps(key_data, sort_keys=True)
        return hashlib.md5(key_string.encode()).hexdigest()

    async def check_duplicate(self, request: Request) -> Optional[Any]:
        """æ£€æŸ¥é‡å¤è¯·æ±‚"""
        request_key = self._generate_request_key(request)

        # å¦‚æœè¯·æ±‚æ­£åœ¨å¤„ç†ä¸­ï¼Œç­‰å¾…ç»“æœ
        if request_key in self.processing_requests:
            logger.info(f"ğŸ”„ æ£€æµ‹åˆ°é‡å¤è¯·æ±‚ï¼Œç­‰å¾…å¤„ç†: {request_key}")

            # ç­‰å¾…è¯·æ±‚å®Œæˆï¼ˆæœ€å¤šç­‰å¾…30ç§’ï¼‰
            for _ in range(300):  # 30ç§’ï¼Œæ¯100msæ£€æŸ¥ä¸€æ¬¡
                if request_key in self.request_results:
                    result = self.request_results[request_key]
                    # æ¸…ç†è¿‡æœŸç»“æœ
                    del self.request_results[request_key]
                    del self.request_timestamps[request_key]
                    return result

                await asyncio.sleep(0.1)

            logger.warning(f"âš ï¸ é‡å¤è¯·æ±‚ç­‰å¾…è¶…æ—¶: {request_key}")

        return None

    async def mark_processing(self, request: Request):
        """æ ‡è®°è¯·æ±‚å¼€å§‹å¤„ç†"""
        request_key = self._generate_request_key(request)
        self.processing_requests.add(request_key)

    async def mark_completed(self, request: Request):
        """æ ‡è®°è¯·æ±‚å¤„ç†å®Œæˆ"""
        request_key = self._generate_request_key(request)
        self.processing_requests.discard(request_key)

    async def cache_result(self, request: Request, result: Any):
        """ç¼“å­˜è¯·æ±‚ç»“æœ"""
        request_key = self._generate_request_key(request)
        self.request_results[request_key] = result
        self.request_timestamps[request_key] = time.time()

        # æ¸…ç†è¿‡æœŸç»“æœï¼ˆ5åˆ†é’Ÿåï¼‰
        await self._cleanup_expired_results()

    async def _cleanup_expired_results(self):
        """æ¸…ç†è¿‡æœŸç»“æœ"""
        current_time = time.time()
        expired_keys = []

        for key, timestamp in self.request_timestamps.items():
            if current_time - timestamp > 300:  # 5åˆ†é’Ÿ
                expired_keys.append(key)

        for key in expired_keys:
            del self.request_results[key]
            del self.request_timestamps[key]

# ä½¿ç”¨ç¤ºä¾‹
"""
api_optimizer = APIOptimizer()

@api_optimizer.cache_response(ttl=600)
@api_optimizer.rate_limit(requests_per_minute=100)
@api_optimizer.deduplicate_request()
@api_optimizer.compress_response(threshold=2048)
async def generate_visualization(request: Request, data: dict):
    # å¤„ç†å¯è§†åŒ–ç”Ÿæˆé€»è¾‘
    pass
"""

# æ€§èƒ½ç»Ÿè®¡
class PerformanceStats:
    """æ€§èƒ½ç»Ÿè®¡"""

    def __init__(self):
        self.request_counts = defaultdict(int)
        self.response_times = defaultdict(list)
        self.cache_hits = 0
        self.cache_misses = 0
        self.rate_limited_requests = 0

    def record_request(self, endpoint: str, response_time: float):
        """è®°å½•è¯·æ±‚"""
        self.request_counts[endpoint] += 1
        self.response_times[endpoint].append(response_time)

        # ä¿æŒæœ€è¿‘1000ä¸ªå“åº”æ—¶é—´è®°å½•
        if len(self.response_times[endpoint]) > 1000:
            self.response_times[endpoint] = self.response_times[endpoint][-1000:]

    def record_cache_hit(self):
        """è®°å½•ç¼“å­˜å‘½ä¸­"""
        self.cache_hits += 1

    def record_cache_miss(self):
        """è®°å½•ç¼“å­˜æœªå‘½ä¸­"""
        self.cache_misses += 1

    def record_rate_limit(self):
        """è®°å½•é™æµ"""
        self.rate_limited_requests += 1

    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        total_requests = sum(self.request_counts.values())
        cache_hit_rate = (self.cache_hits / (self.cache_hits + self.cache_misses) * 100) if (self.cache_hits + self.cache_misses) > 0 else 0

        avg_response_times = {}
        for endpoint, times in self.response_times.items():
            if times:
                avg_response_times[endpoint] = sum(times) / len(times)

        return {
            "total_requests": total_requests,
            "requests_per_endpoint": dict(self.request_counts),
            "average_response_times": avg_response_times,
            "cache_hit_rate": f"{cache_hit_rate:.2f}%",
            "cache_hits": self.cache_hits,
            "cache_misses": self.cache_misses,
            "rate_limited_requests": self.rate_limited_requests,
            "timestamp": time.time()
        }

# å…¨å±€å®ä¾‹
performance_stats = PerformanceStats()