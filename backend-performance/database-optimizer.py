"""
Database Performance Optimizer
æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–å™¨

åŠŸèƒ½åŒ…æ‹¬ï¼š
- è¿æ¥æ± ç®¡ç†
- æŸ¥è¯¢ä¼˜åŒ–
- ç´¢å¼•å»ºè®®
- ç¼“å­˜ç­–ç•¥
- æ‰¹é‡æ“ä½œä¼˜åŒ–
"""

import asyncio
import time
import json
import logging
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from contextlib import asynccontextmanager
from sqlalchemy import text, create_engine, MetaData, inspect
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker
from sqlalchemy.pool import QueuePool
from sqlalchemy.orm import sessionmaker
import aioredis
from functools import wraps
import hashlib

logger = logging.getLogger(__name__)

@dataclass
class DatabaseConfig:
    """æ•°æ®åº“é…ç½®"""
    url: str = "sqlite+aiosqlite:///./visualization.db"
    pool_size: int = 20
    max_overflow: int = 30
    pool_timeout: int = 30
    pool_recycle: int = 3600
    echo: bool = False
    connection_timeout: int = 60

@dataclass
class QueryOptimization:
    """æŸ¥è¯¢ä¼˜åŒ–ç»“æœ"""
    original_query: str
    optimized_query: str
    suggestions: List[str] = field(default_factory=list)
    estimated_improvement: float = 0.0
    added_indexes: List[str] = field(default_factory=list)

class DatabaseOptimizer:
    """æ•°æ®åº“ä¼˜åŒ–å™¨ä¸»ç±»"""

    def __init__(self, config: DatabaseConfig = None):
        self.config = config or DatabaseConfig()
        self.engine = None
        self.async_session_factory = None
        self.redis = None
        self.query_cache = {}
        self.query_stats = {}
        self.slow_queries = []
        self.index_suggestions = []

    async def initialize(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥"""
        await self._create_engine()
        await self._setup_redis()
        await self._analyze_schema()
        logger.info("ğŸš€ æ•°æ®åº“ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")

    async def _create_engine(self):
        """åˆ›å»ºæ•°æ®åº“å¼•æ“"""
        try:
            # åˆ›å»ºå¼‚æ­¥å¼•æ“
            self.engine = create_async_engine(
                self.config.url,
                poolclass=QueuePool,
                pool_size=self.config.pool_size,
                max_overflow=self.config.max_overflow,
                pool_timeout=self.config.pool_timeout,
                pool_recycle=self.config.pool_recycle,
                echo=self.config.echo,
                connect_args={"timeout": self.config.connection_timeout}
            )

            # åˆ›å»ºä¼šè¯å·¥å‚
            self.async_session_factory = async_sessionmaker(
                self.engine,
                class_=AsyncSession,
                expire_on_commit=False
            )

            logger.info("âœ… æ•°æ®åº“å¼•æ“åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“å¼•æ“åˆ›å»ºå¤±è´¥: {e}")
            raise

    async def _setup_redis(self):
        """è®¾ç½®Redisç¼“å­˜"""
        try:
            self.redis = await aioredis.from_url("redis://localhost:6379", decode_responses=True)
            logger.info("âœ… Redisç¼“å­˜è¿æ¥æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ Redisè¿æ¥å¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°ç¼“å­˜: {e}")

    async def _analyze_schema(self):
        """åˆ†ææ•°æ®åº“æ¨¡å¼"""
        async with self.get_session() as session:
            try:
                # è·å–è¡¨ä¿¡æ¯
                inspector = inspect(self.engine.sync_engine if hasattr(self.engine, 'sync_engine') else self.engine)
                tables = inspector.get_table_names()

                for table_name in tables:
                    # åˆ†æè¡¨çš„ç´¢å¼•
                    indexes = inspector.get_indexes(table_name)
                    columns = inspector.get_columns(table_name)

                    # ç”Ÿæˆç´¢å¼•å»ºè®®
                    await self._generate_index_suggestions(table_name, columns, indexes)

                logger.info(f"âœ… æ•°æ®åº“æ¨¡å¼åˆ†æå®Œæˆï¼Œå…±{len(tables)}å¼ è¡¨")
            except Exception as e:
                logger.warning(f"âš ï¸ æ•°æ®åº“æ¨¡å¼åˆ†æå¤±è´¥: {e}")

    async def _generate_index_suggestions(self, table_name: str, columns: List[Dict], existing_indexes: List[Dict]):
        """ç”Ÿæˆç´¢å¼•å»ºè®®"""
        # å¸¸è§éœ€è¦ç´¢å¼•çš„åˆ—ç±»å‹
        index_candidate_types = ['varchar', 'text', 'integer', 'timestamp']

        # æŸ¥æ‰¾å¯èƒ½éœ€è¦ç´¢å¼•çš„åˆ—
        for column in columns:
            column_name = column['name']
            column_type = column['type'].lower()

            # è·³è¿‡å·²æœ‰ç´¢å¼•çš„åˆ—
            if any(column_name in idx['column_names'] for idx in existing_indexes):
                continue

            # ä¸ºç‰¹å®šç±»å‹åˆ—å»ºè®®ç´¢å¼•
            if any(col_type in column_type for col_type in index_candidate_types):
                # æ£€æŸ¥æ˜¯å¦æ˜¯å¤–é”®æˆ–å¸¸ç”¨æŸ¥è¯¢åˆ—
                if (column_name.endswith('_id') or
                    column_name in ['name', 'type', 'status', 'created_at', 'updated_at'] or
                    'user' in column_name.lower()):

                    suggestion = f"CREATE INDEX idx_{table_name}_{column_name} ON {table_name} ({column_name})"
                    if suggestion not in self.index_suggestions:
                        self.index_suggestions.append(suggestion)

    @asynccontextmanager
    async def get_session(self):
        """è·å–æ•°æ®åº“ä¼šè¯"""
        if not self.async_session_factory:
            raise RuntimeError("æ•°æ®åº“æœªåˆå§‹åŒ–")

        async with self.async_session_factory() as session:
            try:
                yield session
                await session.commit()
            except Exception:
                await session.rollback()
                raise

    def cache_query_result(self, ttl: int = 300, key_prefix: str = "db_cache"):
        """æŸ¥è¯¢ç»“æœç¼“å­˜è£…é¥°å™¨"""
        def decorator(func):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                # ç”Ÿæˆç¼“å­˜é”®
                cache_key = f"{key_prefix}:{self._generate_query_cache_key(func.__name__, args, kwargs)}"

                # å°è¯•ä»ç¼“å­˜è·å–
                cached_result = await self._get_cached_result(cache_key)
                if cached_result is not None:
                    logger.info(f"ğŸ’¾ æŸ¥è¯¢ç¼“å­˜å‘½ä¸­: {func.__name__}")
                    return cached_result

                # æ‰§è¡ŒæŸ¥è¯¢
                start_time = time.time()
                result = await func(*args, **kwargs)
                execution_time = time.time() - start_time

                # è®°å½•æŸ¥è¯¢ç»Ÿè®¡
                await self._record_query_stats(func.__name__, execution_time)

                # ç¼“å­˜ç»“æœ
                await self._cache_result(cache_key, result, ttl)

                # æ£€æŸ¥æ…¢æŸ¥è¯¢
                if execution_time > 1.0:  # è¶…è¿‡1ç§’çš„æŸ¥è¯¢
                    await self._record_slow_query(func.__name__, args, kwargs, execution_time)

                return result

            return wrapper
        return decorator

    async def _generate_query_cache_key(self, func_name: str, args: tuple, kwargs: dict) -> str:
        """ç”ŸæˆæŸ¥è¯¢ç¼“å­˜é”®"""
        # åºåˆ—åŒ–å‚æ•°
        try:
            args_str = json.dumps(args, sort_keys=True, default=str)
            kwargs_str = json.dumps(kwargs, sort_keys=True, default=str)
        except (TypeError, ValueError):
            # å¦‚æœåºåˆ—åŒ–å¤±è´¥ï¼Œä½¿ç”¨å­—ç¬¦ä¸²è¡¨ç¤º
            args_str = str(args)
            kwargs_str = str(kwargs)

        # ç”Ÿæˆå“ˆå¸Œ
        key_data = f"{func_name}:{args_str}:{kwargs_str}"
        return hashlib.md5(key_data.encode()).hexdigest()

    async def _get_cached_result(self, cache_key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜ç»“æœ"""
        if self.redis:
            try:
                cached_data = await self.redis.get(cache_key)
                if cached_data:
                    return json.loads(cached_data)
            except Exception as e:
                logger.warning(f"Redisç¼“å­˜è¯»å–å¤±è´¥: {e}")

        # å›é€€åˆ°æœ¬åœ°ç¼“å­˜
        return self.query_cache.get(cache_key)

    async def _cache_result(self, cache_key: str, result: Any, ttl: int):
        """ç¼“å­˜æŸ¥è¯¢ç»“æœ"""
        # å°è¯•ç¼“å­˜åˆ°Redis
        if self.redis:
            try:
                await self.redis.setex(cache_key, ttl, json.dumps(result, default=str))
                return
            except Exception as e:
                logger.warning(f"Redisç¼“å­˜å†™å…¥å¤±è´¥: {e}")

        # å›é€€åˆ°æœ¬åœ°ç¼“å­˜
        self.query_cache[cache_key] = result

        # é™åˆ¶æœ¬åœ°ç¼“å­˜å¤§å°
        if len(self.query_cache) > 1000:
            # åˆ é™¤æœ€æ—§çš„ä¸€åŠç¼“å­˜
            keys_to_remove = list(self.query_cache.keys())[:500]
            for key in keys_to_remove:
                del self.query_cache[key]

    async def _record_query_stats(self, query_name: str, execution_time: float):
        """è®°å½•æŸ¥è¯¢ç»Ÿè®¡"""
        if query_name not in self.query_stats:
            self.query_stats[query_name] = {
                'count': 0,
                'total_time': 0.0,
                'avg_time': 0.0,
                'min_time': float('inf'),
                'max_time': 0.0
            }

        stats = self.query_stats[query_name]
        stats['count'] += 1
        stats['total_time'] += execution_time
        stats['avg_time'] = stats['total_time'] / stats['count']
        stats['min_time'] = min(stats['min_time'], execution_time)
        stats['max_time'] = max(stats['max_time'], execution_time)

    async def _record_slow_query(self, query_name: str, args: tuple, kwargs: dict, execution_time: float):
        """è®°å½•æ…¢æŸ¥è¯¢"""
        slow_query = {
            'query_name': query_name,
            'args': str(args)[:200],  # é™åˆ¶é•¿åº¦
            'kwargs': str(kwargs)[:200],
            'execution_time': execution_time,
            'timestamp': time.time()
        }

        self.slow_queries.append(slow_query)

        # ä¿æŒæœ€è¿‘100ä¸ªæ…¢æŸ¥è¯¢è®°å½•
        if len(self.slow_queries) > 100:
            self.slow_queries = self.slow_queries[-100:]

        logger.warning(f"ğŸŒ æ…¢æŸ¥è¯¢æ£€æµ‹: {query_name} è€—æ—¶ {execution_time:.2f}s")

    async def execute_optimized_query(self, query: str, params: Dict = None) -> List[Dict]:
        """æ‰§è¡Œä¼˜åŒ–çš„æŸ¥è¯¢"""
        async with self.get_session() as session:
            try:
                # åˆ†æå¹¶ä¼˜åŒ–æŸ¥è¯¢
                optimized_query = await self._optimize_query(query)

                # æ‰§è¡ŒæŸ¥è¯¢
                result = await session.execute(text(optimized_query), params or {})
                rows = result.fetchall()

                # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                return [dict(row._mapping) for row in rows]

            except Exception as e:
                logger.error(f"âŒ æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}")
                raise

    async def _optimize_query(self, query: str) -> str:
        """ä¼˜åŒ–SQLæŸ¥è¯¢"""
        optimized_query = query.strip()

        # åŸºæœ¬æŸ¥è¯¢ä¼˜åŒ–å»ºè®®
        optimizations = [
            # é¿å…SELECT *
            ("SELECT *", "SELECT specific_columns"),
            # æ·»åŠ LIMITå­å¥ï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
            ("SELECT", "SELECT"),
            # ä½¿ç”¨ç´¢å¼•æç¤ºï¼ˆå¦‚æœé€‚ç”¨ï¼‰
            ("FROM", "FROM"),
        ]

        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„æŸ¥è¯¢ä¼˜åŒ–é€»è¾‘
        # æ¯”å¦‚æŸ¥è¯¢è®¡åˆ’åˆ†æã€ç´¢å¼•ä½¿ç”¨å»ºè®®ç­‰

        return optimized_query

    async def batch_insert(self, table_name: str, data: List[Dict], batch_size: int = 1000) -> int:
        """æ‰¹é‡æ’å…¥ä¼˜åŒ–"""
        if not data:
            return 0

        total_inserted = 0

        async with self.get_session() as session:
            try:
                # åˆ†æ‰¹æ’å…¥
                for i in range(0, len(data), batch_size):
                    batch = data[i:i + batch_size]

                    # æ„å»ºæ‰¹é‡æ’å…¥è¯­å¥
                    columns = list(batch[0].keys())
                    placeholders = ", ".join([f":{col}" for col in columns])
                    query = f"""
                        INSERT INTO {table_name} ({', '.join(columns)})
                        VALUES ({placeholders})
                    """

                    await session.execute(text(query), batch)
                    total_inserted += len(batch)

                    logger.info(f"æ‰¹é‡æ’å…¥: {table_name} {len(batch)} æ¡è®°å½•")

                return total_inserted

            except Exception as e:
                logger.error(f"âŒ æ‰¹é‡æ’å…¥å¤±è´¥: {e}")
                raise

    async def bulk_update(self, table_name: str, data: List[Dict], key_column: str = 'id', batch_size: int = 1000) -> int:
        """æ‰¹é‡æ›´æ–°ä¼˜åŒ–"""
        if not data:
            return 0

        total_updated = 0

        async with self.get_session() as session:
            try:
                # åˆ†æ‰¹æ›´æ–°
                for i in range(0, len(data), batch_size):
                    batch = data[i:i + batch_size]

                    for row in batch:
                        # æ„å»ºæ›´æ–°è¯­å¥
                        key_value = row[key_column]
                        update_columns = {k: v for k, v in row.items() if k != key_column}

                        if update_columns:
                            set_clause = ", ".join([f"{col} = :{col}" for col in update_columns])
                            query = f"""
                                UPDATE {table_name}
                                SET {set_clause}
                                WHERE {key_column} = :{key_column}
                            """

                            await session.execute(text(query), {**update_columns, key_column: key_value})
                            total_updated += 1

                    logger.info(f"æ‰¹é‡æ›´æ–°: {table_name} {len(batch)} æ¡è®°å½•")

                return total_updated

            except Exception as e:
                logger.error(f"âŒ æ‰¹é‡æ›´æ–°å¤±è´¥: {e}")
                raise

    async def analyze_table_performance(self, table_name: str) -> Dict:
        """åˆ†æè¡¨æ€§èƒ½"""
        async with self.get_session() as session:
            try:
                # è·å–è¡¨ç»Ÿè®¡ä¿¡æ¯
                stats_query = f"""
                    SELECT
                        COUNT(*) as row_count,
                        AVG(LENGTH(CAST(* AS TEXT))) as avg_row_size
                    FROM {table_name}
                """

                result = await session.execute(text(stats_query))
                stats = result.fetchone()

                # è·å–ç´¢å¼•ä¿¡æ¯
                index_query = f"""
                    SELECT
                        indexname,
                        indexdef
                    FROM pg_indexes
                    WHERE tablename = '{table_name}'
                """

                try:
                    index_result = await session.execute(text(index_query))
                    indexes = [dict(row._mapping) for row in index_result.fetchall()]
                except:
                    # å¦‚æœä¸æ˜¯PostgreSQLï¼Œä½¿ç”¨é€šç”¨æŸ¥è¯¢
                    indexes = []

                return {
                    'table_name': table_name,
                    'row_count': stats[0] if stats else 0,
                    'avg_row_size': stats[1] if stats else 0,
                    'indexes': indexes,
                    'index_suggestions': self._get_table_index_suggestions(table_name)
                }

            except Exception as e:
                logger.error(f"âŒ è¡¨æ€§èƒ½åˆ†æå¤±è´¥: {e}")
                return {'error': str(e)}

    def _get_table_index_suggestions(self, table_name: str) -> List[str]:
        """è·å–è¡¨çš„ç´¢å¼•å»ºè®®"""
        return [suggestion for suggestion in self.index_suggestions if table_name in suggestion]

    async def get_performance_report(self) -> Dict:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        return {
            'query_statistics': self.query_stats,
            'slow_queries': self.slow_queries[-10:],  # æœ€è¿‘10ä¸ªæ…¢æŸ¥è¯¢
            'index_suggestions': self.index_suggestions,
            'cache_stats': {
                'local_cache_size': len(self.query_cache),
                'redis_connected': self.redis is not None
            },
            'top_slow_queries': self._get_top_slow_queries(),
            'query_performance_summary': self._get_query_performance_summary()
        }

    def _get_top_slow_queries(self) -> List[Dict]:
        """è·å–æœ€æ…¢çš„æŸ¥è¯¢"""
        return sorted(self.slow_queries, key=lambda x: x['execution_time'], reverse=True)[:10]

    def _get_query_performance_summary(self) -> Dict:
        """è·å–æŸ¥è¯¢æ€§èƒ½æ‘˜è¦"""
        if not self.query_stats:
            return {}

        total_queries = sum(stats['count'] for stats in self.query_stats.values())
        total_time = sum(stats['total_time'] for stats in self.query_stats.values())
        avg_query_time = total_time / total_queries if total_queries > 0 else 0

        slowest_query = max(self.query_stats.items(), key=lambda x: x[1]['max_time'])
        fastest_query = min(self.query_stats.items(), key=lambda x: x[1]['min_time'])

        return {
            'total_queries': total_queries,
            'total_time': total_time,
            'average_query_time': avg_query_time,
            'slowest_query': {
                'name': slowest_query[0],
                'time': slowest_query[1]['max_time']
            },
            'fastest_query': {
                'name': fastest_query[0],
                'time': fastest_query[1]['min_time']
            }
        }

    async def cleanup_cache(self):
        """æ¸…ç†ç¼“å­˜"""
        # æ¸…ç†æœ¬åœ°ç¼“å­˜
        self.query_cache.clear()

        # æ¸…ç†Redisç¼“å­˜
        if self.redis:
            try:
                keys = await self.redis.keys("db_cache:*")
                if keys:
                    await self.redis.delete(*keys)
                    logger.info(f"ğŸ—‘ï¸ æ¸…ç†Redisç¼“å­˜: {len(keys)} ä¸ªé”®")
            except Exception as e:
                logger.warning(f"Redisç¼“å­˜æ¸…ç†å¤±è´¥: {e}")

        logger.info("ğŸ§¹ ç¼“å­˜æ¸…ç†å®Œæˆ")

    async def close(self):
        """å…³é—­è¿æ¥"""
        if self.engine:
            await self.engine.dispose()

        if self.redis:
            await self.redis.close()

        logger.info("ğŸ”Œ æ•°æ®åº“è¿æ¥å·²å…³é—­")

# ä½¿ç”¨ç¤ºä¾‹
"""
db_optimizer = DatabaseOptimizer()
await db_optimizer.initialize()

@db_optimizer.cache_query_result(ttl=600)
async def get_user_visualizations(user_id: int):
    async with db_optimizer.get_session() as session:
        result = await session.execute(
            text("SELECT * FROM visualizations WHERE user_id = :user_id"),
            {"user_id": user_id}
        )
        return [dict(row._mapping) for row in result.fetchall()]
"""

# è¿æ¥æ± ç›‘æ§å™¨
class ConnectionPoolMonitor:
    """è¿æ¥æ± ç›‘æ§å™¨"""

    def __init__(self, engine):
        self.engine = engine
        self.pool_stats = {}

    async def get_pool_stats(self) -> Dict:
        """è·å–è¿æ¥æ± ç»Ÿè®¡"""
        if hasattr(self.engine, 'pool'):
            pool = self.engine.pool
            return {
                'size': pool.size(),
                'checked_in': pool.checkedin(),
                'checked_out': pool.checkedout(),
                'overflow': pool.overflow(),
                'invalid': pool.invalid()
            }
        return {}

    async def monitor_pool_health(self):
        """ç›‘æ§è¿æ¥æ± å¥åº·çŠ¶æ€"""
        stats = await self.get_pool_stats()

        if stats:
            # æ£€æŸ¥è¿æ¥æ± ä½¿ç”¨ç‡
            usage_rate = (stats['checked_out'] / stats['size']) * 100 if stats['size'] > 0 else 0

            if usage_rate > 80:
                logger.warning(f"âš ï¸ è¿æ¥æ± ä½¿ç”¨ç‡è¿‡é«˜: {usage_rate:.1f}%")

            # æ£€æŸ¥æº¢å‡ºè¿æ¥
            if stats['overflow'] > 0:
                logger.warning(f"âš ï¸ å­˜åœ¨æº¢å‡ºè¿æ¥: {stats['overflow']}")

            # æ£€æŸ¥æ— æ•ˆè¿æ¥
            if stats['invalid'] > 0:
                logger.warning(f"âš ï¸ å­˜åœ¨æ— æ•ˆè¿æ¥: {stats['invalid']}")

        return stats