"""
ä¸‡ç‰©å¯è§†åŒ– v2.0 - ç»Ÿä¸€å¤§æ¨¡å‹å®¢æˆ·ç«¯
æ”¯æŒå¤šç§LLMæä¾›å•†çš„çµæ´»è°ƒç”¨
"""

from typing import Dict, Any, Optional, List
from abc import ABC, abstractmethod
import json
import aiohttp
import asyncio
from datetime import datetime

class LLMProvider(str):
    """LLMæä¾›å•†æšä¸¾"""
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    AZURE = "azure"
    HUGGINGFACE = "huggingface"
    LOCAL = "local"
    CUSTOM = "custom"

class LLMConfig:
    """LLMé…ç½®ç±»"""
    def __init__(
        self,
        provider: LLMProvider,
        model_name: str,
        api_key: str,
        base_url: Optional[str] = None,
        max_tokens: int = 4000,
        temperature: float = 0.7,
        timeout: int = 60,
        custom_headers: Optional[Dict[str, str]] = None,
        organization_id: Optional[str] = None
    ):
        self.provider = provider
        self.model_name = model_name
        self.api_key = api_key
        self.base_url = base_url
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.timeout = timeout
        self.custom_headers = custom_headers or {}
        self.organization_id = organization_id

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "provider": self.provider.value,
            "model_name": self.model_name,
            "api_key": self.api_key,
            "base_url": self.base_url,
            "max_tokens": self.max_tokens,
            "temperature": self.temperature,
            "timeout": self.timeout,
            "custom_headers": self.custom_headers,
            "organization_id": self.organization_id
        }

class LLMResponse:
    """LLMå“åº”ç±»"""
    def __init__(
        self,
        content: str,
        success: bool = True,
        error: Optional[str] = None,
        usage: Optional[Dict[str, Any]] = None,
        metadata: Optional[Dict[str, Any]] = None
    ):
        self.content = content
        self.success = success
        self.error = error
        self.usage = usage or {}
        self.metadata = metadata or {}

class BaseLLMClient(ABC):
    """LLMå®¢æˆ·ç«¯åŸºç±»"""

    def __init__(self, config: LLMConfig):
        self.config = config
        self.provider = config.provider

    @abstractmethod
    async def generate(self, prompt: str, **kwargs) -> LLMResponse:
        """ç”Ÿæˆå†…å®¹çš„æ ¸å¿ƒæ–¹æ³•"""
        pass

    @abstractmethod
    async def validate_connection(self) -> bool:
        """éªŒè¯è¿æ¥æ˜¯å¦å¯ç”¨"""
        pass

    @abstractmethod
    def get_provider_info(self) -> Dict[str, Any]:
        """è·å–æä¾›å•†ä¿¡æ¯"""
        pass

class OpenAIClient(BaseLLMClient):
    """OpenAIå®¢æˆ·ç«¯å®ç°"""

    def __init__(self, config: LLMConfig):
        super().__init__(config)
        self.base_url = config.base_url or "https://api.openai.com/v1"

    async def generate(self, prompt: str, **kwargs) -> LLMResponse:
        """OpenAIç”Ÿæˆæ–¹æ³•"""
        headers = {
            "Authorization": f"Bearer {self.config.api_key}",
            "Content-Type": "application/json",
            **self.config.custom_headers
        }

        if self.config.organization_id:
            headers["OpenAI-Organization"] = self.config.organization_id

        payload = {
            "model": self.config.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": kwargs.get("max_tokens", self.config.max_tokens),
            "temperature": kwargs.get("temperature", self.config.temperature)
        }

        try:
            async with aiohttp.ClientSession(headers=headers, timeout=aiohttp.ClientTimeout(total=self.config.timeout)) as session:
                async with session.post(f"{self.base_url}/chat/completions", json=payload) as response:
                    if response.status == 200:
                        result = await response.json()
                        content = result["choices"][0]["message"]["content"]
                        usage = result.get("usage", {})
                        return LLMResponse(
                            content=content,
                            usage=usage,
                            metadata={"model": self.config.model_name, "provider": "openai"}
                        )
                    else:
                        error_text = await response.text()
                        return LLMResponse(
                            success=False,
                            error=f"OpenAI APIé”™è¯¯: {response.status} - {error_text}"
                        )
        except Exception as e:
            return LLMResponse(
                success=False,
                error=f"OpenAIå®¢æˆ·ç«¯é”™è¯¯: {str(e)}"
            )

    async def validate_connection(self) -> bool:
        """éªŒè¯OpenAIè¿æ¥"""
        try:
            headers = {
                "Authorization": f"Bearer {self.config.api_key}",
                "Content-Type": "application/json"
            }

            async with aiohttp.ClientSession(headers=headers, timeout=10) as session:
                async with session.get(f"{self.base_url}/models") as response:
                    return response.status == 200
        except:
            return False

    def get_provider_info(self) -> Dict[str, Any]:
        """è·å–OpenAIæä¾›å•†ä¿¡æ¯"""
        return {
            "provider": "openai",
            "models": ["gpt-3.5-turbo", "gpt-4", "gpt-4-turbo"],
            "capabilities": ["chat", "function_calling", "vision"],
            "pricing": "per-token",
            "context_window": 128000
        }

class AnthropicClient(BaseLLMClient):
    """Anthropic Claudeå®¢æˆ·ç«¯å®ç°"""

    def __init__(self, config: LLMConfig):
        super().__init__(config)
        self.base_url = config.base_url or "https://api.anthropic.com"

    async def generate(self, prompt: str, **kwargs) -> LLMResponse:
        """Anthropicç”Ÿæˆæ–¹æ³•"""
        headers = {
            "x-api-key": self.config.api_key,
            "Content-Type": "application/json",
            "anthropic-version": "2023-06-01",
            **self.config.custom_headers
        }

        payload = {
            "model": self.config.model_name,
            "max_tokens": kwargs.get("max_tokens", self.config.max_tokens),
            "temperature": kwargs.get("temperature", self.config.temperature),
            "messages": [{"role": "user", "content": prompt}]
        }

        try:
            async with aiohttp.ClientSession(headers=headers, timeout=aiohttp.ClientTimeout(total=self.config.timeout)) as session:
                async with session.post(f"{self.base_url}/v1/messages", json=payload) as response:
                    if response.status == 200:
                        result = await response.json()
                        content = result["content"][0]["text"]
                        usage = result.get("usage", {})
                        return LLMResponse(
                            content=content,
                            usage=usage,
                            metadata={"model": self.config.model_name, "provider": "anthropic"}
                        )
                    else:
                        error_text = await response.text()
                        return LLMResponse(
                            success=False,
                            error=f"Anthropic APIé”™è¯¯: {response.status} - {error_text}"
                        )
        except Exception as e:
            return LLMResponse(
                success=False,
                error=f"Anthropicå®¢æˆ·ç«¯é”™è¯¯: {str(e)}"
            )

    async def validate_connection(self) -> bool:
        """éªŒè¯Anthropicè¿æ¥"""
        try:
            headers = {"x-api-key": self.config.api_key}
            async with aiohttp.ClientSession(headers=headers, timeout=10) as session:
                async with session.get(f"{self.base_url}/v1/messages") as response:
                    return response.status in [200, 400]  # 400å¯èƒ½æ˜¯æœªæˆæƒä½†APIå­˜åœ¨
        except:
            return False

    def get_provider_info(self) -> Dict[str, Any]:
        """è·å–Anthropicæä¾›å•†ä¿¡æ¯"""
        return {
            "provider": "anthropic",
            "models": ["claude-3-sonnet-20240229", "claude-3-opus-20240229"],
            "capabilities": ["chat", "vision", "function_calling"],
            "pricing": "per-token",
            "context_window": 200000
        }

class CustomLLMClient(BaseLLMClient):
    """è‡ªå®šä¹‰APIå®¢æˆ·ç«¯å®ç°"""

    def __init__(self, config: LLMConfig):
        super().__init__(config)
        self.base_url = config.base_url

    async def generate(self, prompt: str, **kwargs) -> LLMResponse:
        """è‡ªå®šä¹‰APIç”Ÿæˆæ–¹æ³•"""
        headers = {
            "Authorization": f"Bearer {self.config.api_key}",
            "Content-Type": "application/json",
            **self.config.custom_headers
        }

        # æ ¹æ®ä¸åŒè‡ªå®šä¹‰APIè°ƒæ•´payload
        if self.config.provider == "deepseek":
            payload = {
                "model": self.config.model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": kwargs.get("max_tokens", self.config.max_tokens),
                "temperature": kwargs.get("temperature", self.config.temperature),
                "stream": False
            }
        elif self.config.provider == "qwen":
            payload = {
                "model": self.config.model_name,
                "input": {"messages": [{"role": "user", "content": prompt}]},
                "parameters": {
                    "max_tokens": kwargs.get("max_tokens", self.config.max_tokens),
                    "temperature": kwargs.get("temperature", self.config.temperature)
                }
            }
        else:
            # é»˜è®¤é€šç”¨æ ¼å¼
            payload = {
                "prompt": prompt,
                "model": self.config.model_name,
                "max_tokens": kwargs.get("max_tokens", self.config.max_tokens),
                "temperature": kwargs.get("temperature", self.config.temperature)
            }

        try:
            async with aiohttp.ClientSession(headers=headers, timeout=aiohttp.ClientTimeout(total=self.config.timeout)) as session:
                async with session.post(f"{self.base_url}/v1/chat/completions", json=payload) as response:
                    if response.status == 200:
                        result = await response.json()
                        # ä¸åŒAPIè¿”å›æ ¼å¼éœ€è¦é€‚é…
                        if "choices" in result:
                            content = result["choices"][0]["message"]["content"]
                        elif "output" in result:
                            content = result["output"]["text"]
                        else:
                            content = str(result)

                        usage = result.get("usage", {})
                        return LLMResponse(
                            content=content,
                            usage=usage,
                            metadata={"model": self.config.model_name, "provider": self.config.provider}
                        )
                    else:
                        error_text = await response.text()
                        return LLMResponse(
                            success=False,
                            error=f"è‡ªå®šä¹‰APIé”™è¯¯: {response.status} - {error_text}"
                        )
        except Exception as e:
            return LLMResponse(
                success=False,
                error=f"è‡ªå®šä¹‰APIå®¢æˆ·ç«¯é”™è¯¯: {str(e)}"
            )

    async def validate_connection(self) -> bool:
        """éªŒè¯è‡ªå®šä¹‰APIè¿æ¥"""
        try:
            headers = {"Authorization": f"Bearer {self.config.api_key}"}
            async with aiohttp.ClientSession(headers=headers, timeout=10) as session:
                async with session.get(f"{self.base_url}/v1/models") as response:
                    return response.status == 200
        except:
            return False

    def get_provider_info(self) -> Dict[str, Any]:
        """è·å–è‡ªå®šä¹‰APIæä¾›å•†ä¿¡æ¯"""
        return {
            "provider": self.config.provider,
            "models": [self.config.model_name],
            "capabilities": ["chat", "custom"],
            "pricing": "unknown",
            "context_window": "unknown"
        }

class LLMClientFactory:
    """LLMå®¢æˆ·ç«¯å·¥å‚ç±»"""

    @staticmethod
    def create_client(config: LLMConfig) -> BaseLLMClient:
        """æ ¹æ®é…ç½®åˆ›å»ºå¯¹åº”çš„å®¢æˆ·ç«¯"""
        if config.provider == LLMProvider.OPENAI:
            return OpenAIClient(config)
        elif config.provider == LLMProvider.ANTHROPIC:
            return AnthropicClient(config)
        elif config.provider in [LLMProvider.DEEPSEEK, LLMProvider.QWEN, LLMProvider.CUSTOM]:
            return CustomLLMClient(config)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {config.provider}")

    @staticmethod
    def get_supported_providers() -> List[str]:
        """è·å–æ‰€æœ‰æ”¯æŒçš„æä¾›å•†"""
        return [provider.value for provider in LLMProvider]

    @staticmethod
    def load_config_from_file(config_path: str) -> Dict[str, LLMConfig]:
        """ä»æ–‡ä»¶åŠ è½½é…ç½®"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config_data = json.load(f)

            configs = {}
            for name, config_dict in config_data.items():
                configs[name] = LLMConfig(**config_dict)

            return configs
        except Exception as e:
            print(f"âŒ åŠ è½½LLMé…ç½®å¤±è´¥: {e}")
            return {}

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ç¤ºä¾‹é…ç½®
    configs = {
        "openai_gpt4": LLMConfig(
            provider=LLMProvider.OPENAI,
            model_name="gpt-4",
            api_key="your-openai-key",
            temperature=0.7
        ),
        "anthropic_claude": LLMConfig(
            provider=LLMProvider.ANTHROPIC,
            model_name="claude-3-sonnet-20240229",
            api_key="your-anthropic-key"
        ),
        "deepseek_chat": LLMConfig(
            provider=LLMProvider.DEEPSEEK,
            model_name="deepseek-chat",
            api_key="your-deepseek-key",
            base_url="https://api.deepseek.com"
        )
    }

    # åˆ›å»ºå®¢æˆ·ç«¯
    openai_client = LLMClientFactory.create_client(configs["openai_gpt4"])
    anthropic_client = LLMClientFactory.create_client(configs["anthropic_claude"])
    deepseek_client = LLMClientFactory.create_client(configs["deepseek_chat"])

    print("âœ… LLMå®¢æˆ·ç«¯ç³»ç»Ÿå·²åˆå§‹åŒ–")
    print(f"ğŸ“‹ æ”¯æŒçš„æä¾›å•†: {LLMClientFactory.get_supported_providers()}")